{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def figsize(w,h):\n",
    "    plt.rcParams['figure.figsize']=[w,h]\n",
    "figsize(15,5) #for big visuals\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.concat(\n",
    "    [\n",
    "        pd.read_csv(\"../resources/test_labels_2_annotators.csv\"),\n",
    "        pd.read_csv(\"../resources/val_labels_2_annotators.csv\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "49 / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../src\")\n",
    "import evaluation\n",
    "\n",
    "\n",
    "def convert(x):\n",
    "    if x == \"u\":\n",
    "        return -1\n",
    "    elif x == \"n\":\n",
    "        return -1\n",
    "    return x\n",
    "\n",
    "\n",
    "labels[\"aiid_label_cc\"] = labels[\"aiid_label_cc\"].apply(convert).astype(int)\n",
    "labels[\"aiid_label_sl\"] = labels[\"aiid_label_sl\"].apply(convert).astype(int)\n",
    "evaluation.evaluate(labels[\"aiid_label_cc\"].values, labels[\"aiid_label_sl\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# choose label correspondence by majority voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which of Sam's labels correspond to each of Chapin's labels?\n",
    "labels_sl = labels[\"aiid_label_sl\"].unique()\n",
    "labels_cc = labels[\"aiid_label_cc\"].unique()\n",
    "label_map_sl_to_cc = {}\n",
    "for label_sl in labels_sl:\n",
    "    # majority voting for Chapin's labels\n",
    "    label_cc = (\n",
    "        labels.loc[labels[\"aiid_label_sl\"] == label_sl, \"aiid_label_cc\"]\n",
    "        .mode()\n",
    "        .values[0]\n",
    "    )\n",
    "    label_map_sl_to_cc[label_sl] = label_cc\n",
    "\n",
    "labels[\"aiid_label_sl_using_cc_ids\"] = labels[\"aiid_label_sl\"].map(label_map_sl_to_cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which of Chapin's labels correspond to each of Sam's labels?\n",
    "label_map_cc_to_sl = {}\n",
    "for label_cc in labels_cc:\n",
    "    # majority voting for Chapin's labels\n",
    "    label_sl = (\n",
    "        labels.loc[labels[\"aiid_label_cc\"] == label_cc, \"aiid_label_sl\"]\n",
    "        .mode()\n",
    "        .values[0]\n",
    "    )\n",
    "    label_map_cc_to_sl[label_cc] = label_sl\n",
    "\n",
    "labels[\"aiid_label_cc_using_sl_ids\"] = labels[\"aiid_label_cc\"].map(label_map_cc_to_sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(\n",
    "    labels[\"aiid_label_cc\"], labels[\"aiid_label_sl_using_cc_ids\"]\n",
    "), accuracy_score(labels[\"aiid_label_sl\"], labels[\"aiid_label_cc_using_sl_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect disagreements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disagreement = labels[\n",
    "    (labels[\"aiid_label_cc\"] != labels[\"aiid_label_sl_using_cc_ids\"])\n",
    "    | (labels[\"aiid_label_sl\"] != labels[\"aiid_label_cc_using_sl_ids\"])\n",
    "]\n",
    "\n",
    "# what if we ignore where one of us put u or n?\n",
    "strong_disagreement = disagreement[\n",
    "    (disagreement.aiid_label_sl > 0) & (disagreement.aiid_label_cc > 0)\n",
    "]\n",
    "len(strong_disagreement[strong_disagreement.nearest])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can throw out any labels where either annotator marked them as 'u' - these are poor samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strong_disagreement[strong_disagreement.nearest][\n",
    "    [\n",
    "        \"event_id\",\n",
    "        \"file\",\n",
    "        \"song_center_time\",\n",
    "        \"aiid_label_cc\",\n",
    "        \"aiid_label_sl_using_cc_ids\",\n",
    "        \"aiid_label_sl\",\n",
    "        \"aiid_label_cc_using_sl_ids\",\n",
    "    ]\n",
    "].to_csv(\"label_conflicts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add resolved labels as the final label\n",
    "Chapin and I both re-reviewed all label conflicts and agreed on final labels for all conflicts. \n",
    "\n",
    "These labels are saved as the 'aiid_label' column in ../resources/train_labels.csv and val_labels.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./resolved_label_conflicts.csv\", index_col=0)\n",
    "resolved_conflicts = df.set_index(\"event_id\")[[\"resolved\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = pd.read_csv(\n",
    "    \"../resources/test_labels_2_annotators.csv\", index_col=0\n",
    ").join(resolved_conflicts)\n",
    "# test_labels.to_csv(\"../resources/test_labels_2_annotators.csv\")\n",
    "test_labels[\"aiid_label\"] = test_labels.apply(\n",
    "    lambda x: x[\"resolved\"] if x[\"resolved\"] == x[\"resolved\"] else x[\"aiid_label_cc\"],\n",
    "    axis=1,\n",
    ")\n",
    "test_labels.to_csv(\"../resources/test_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels = pd.read_csv(\"../resources/val_labels_2_annotators.csv\", index_col=0).join(\n",
    "    resolved_conflicts\n",
    ")\n",
    "# test_labels.to_csv(\"../resources/test_labels_2_annotators.csv\")\n",
    "val_labels[\"aiid_label\"] = val_labels.apply(\n",
    "    lambda x: x[\"resolved\"] if x[\"resolved\"] == x[\"resolved\"] else x[\"aiid_label_cc\"],\n",
    "    axis=1,\n",
    ")\n",
    "val_labels.to_csv(\"../resources/val_labels.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opso0110",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
